{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "from models import *\n",
    "import random\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-9)\n",
    "device = \"cuda\"\n",
    "\n",
    "# study 1 model's training with the new loss function with dist limits\n",
    "\n",
    "# adjustable parameters\n",
    "alpha_d = 100 # IID\n",
    "local_ep = 2 \n",
    "n_clients = 30 # dataset size for one client\n",
    "mali_local_ep = 10\n",
    "global attack \n",
    "attack = \"untargeted\" #\"backdoor\", \"tlp\", \"ut\"\n",
    "model_name = \"ConvNet\" # \"resnet8\", \"ConvNet\"\n",
    "num_classes = 10\n",
    "dataset =\"fmnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(w1, w2):\n",
    "    \"\"\"Compute cosine similarity between two flattened weight tensors\"\"\"\n",
    "    w1_flat, w2_flat = torch.cat([p.view(-1) for p in w1]), torch.cat([p.view(-1) for p in w2])\n",
    "    return 1 - torch.dot(w1_flat, w2_flat) / (torch.norm(w1_flat) * torch.norm(w2_flat))\n",
    "\n",
    "def get_delta_cos(model1, model2, model0_sd):\n",
    "    flat_model0 = flat_dict(model0_sd)\n",
    "    flat_model1 = flat_dict(model1.state_dict())\n",
    "    flat_model2 = flat_dict(model2.state_dict())\n",
    "    \n",
    "    delta = torch.abs(flat_model1 - flat_model2)\n",
    "    org_cos = cos((flat_model1 - flat_model0), (flat_model2 - flat_model0))\n",
    "    return delta, 1-org_cos.item()\n",
    "\n",
    "def model_eval(model, test_loader, attack):\n",
    "    acc = eval_op_ensemble([model], test_loader)\n",
    "    if attack == \"tlp\":\n",
    "        asr = eval_op_ensemble_tr_lf_attack([model], test_loader)\n",
    "    elif attack == \"backdoor\":\n",
    "        asr = eval_op_ensemble_attack([model], test_loader)\n",
    "    elif attack == \"untargeted\":\n",
    "        asr = None\n",
    "    return list(acc.values())[0], list(asr.values())[0]\n",
    "\n",
    "def reverse_train_w_cos(model, loader, optimizer, epochs, model0_sd, model1_sd, beta, budget):    \n",
    "    model.train()\n",
    "\n",
    "    grad_ben = (flat_dict(model1_sd) - flat_dict(model0_sd)).to(device)\n",
    "    \n",
    "    losses = []\n",
    "    # import pdb; pdb.set_trace()\n",
    "    running_loss, samples = 0.0, 0\n",
    "    for ep in range(epochs):\n",
    "        for it, (x, y) in enumerate(loader):\n",
    "            if it % 2 == 0:\n",
    "                losses.append(round(eval_epoch(model, loader), 2))\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss_ce = nn.CrossEntropyLoss()(model(x), y)\n",
    "            # in the untraining reverse the sign of loss\n",
    "            loss_ce = - loss_ce\n",
    "            running_loss += loss_ce.item() * y.shape[0]\n",
    "            samples += y.shape[0]\n",
    "            \n",
    "            # add cos loss \n",
    "            w = torch.cat([p.clone().detach().view(-1) for p in model.parameters()]).to(device)\n",
    "            grad_mail = w - flat_dict(model0_sd)\n",
    "            target = torch.ones(len(w)).to(device)\n",
    "            loss_cos = nn.CosineEmbeddingLoss()(grad_ben.unsqueeze(0), grad_mail.unsqueeze(0), target)\n",
    "            loss_obj = (1-beta) * loss_ce + beta * loss_cos\n",
    "            loss_obj.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"ep{ep}, loss_cs: {loss_ce}, loss_cos: {loss_cos}, loss_obj: {loss_obj}\")\n",
    "        \n",
    "        # break\n",
    "        cos_d = cos_dist(grad_ben, grad_mail)\n",
    "        print(\"eval losses\", losses)\n",
    "        \n",
    "        if cos_d <= budget:\n",
    "            break\n",
    "        \n",
    "\n",
    "    return {\"loss\": running_loss / samples}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      " - Client 0: [202 181 238 193 217 187 220 194 177 186]               -> sum=1995\n",
      " - Client 1: [223 209 214 184 202 193 219 166 212 177]               -> sum=1999\n",
      " - Client 2: [188 214 176 204 179 200 189 220 210 221]               -> sum=2001\n",
      " - Client 3: [215 200 212 168 190 202 210 198 214 193]               -> sum=2002\n",
      " - Client 4: [188 192 202 235 237 164 203 204 178 196]               -> sum=1999\n",
      " - Client 5: [170 181 191 217 198 220 240 192 196 195]               -> sum=2000\n",
      " - Client 6: [200 213 181 219 167 233 185 216 186 199]               -> sum=1999\n",
      " - Client 7: [186 190 179 204 229 189 231 218 200 173]               -> sum=1999\n",
      " - Client 8: [203 189 199 227 226 144 215 194 179 224]               -> sum=2000\n",
      " - Client 9: [182 216 188 175 212 229 196 189 226 188]               -> sum=2001\n",
      ".  .  .  .  .  .  .  .  .  .  \n",
      ".  .  .  .  .  .  .  .  .  .  \n",
      ".  .  .  .  .  .  .  .  .  .  \n",
      " - Client 21: [223 192 212 202 217 173 215 199 186 181]               -> sum=2000\n",
      " - Client 22: [199 190 202 191 215 215 196 196 170 227]               -> sum=2001\n",
      " - Client 23: [228 201 208 175 207 179 222 209 190 180]               -> sum=1999\n",
      " - Client 24: [173 207 224 167 212 200 204 197 208 208]               -> sum=2000\n",
      " - Client 25: [191 199 176 229 187 227 193 188 190 220]               -> sum=2000\n",
      " - Client 26: [190 221 186 228 185 197 191 199 220 182]               -> sum=1999\n",
      " - Client 27: [233 198 205 188 210 213 166 213 184 190]               -> sum=2000\n",
      " - Client 28: [223 200 192 179 211 175 201 213 202 205]               -> sum=2001\n",
      " - Client 29: [178 203 229 226 177 204 166 223 197 202]               -> sum=2005\n",
      " - Total:     [6000 6000 6000 6000 6000 6000 6000 6000 6000 6000]\n",
      "\n",
      "num feat 2048\n",
      "num feat 2048\n",
      "num feat 2048\n",
      "num feat 2048\n"
     ]
    }
   ],
   "source": [
    "# Define transformation (convert images to tensors and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the image with mean and std\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "client_loaders, test_loader, client_data_subsets =\\\n",
    "    data.get_loaders(train_data, test_data, n_clients,\n",
    "                    alpha=alpha_d, batch_size=32, n_data=None, num_workers=4, seed=4)\n",
    "    \n",
    "model_fn = partial(models.get_model(model_name)[\n",
    "                        0], num_classes=num_classes, dataset=dataset)\n",
    "\n",
    "client_loader = client_loaders[0]\n",
    "\n",
    "# created models \n",
    "model0 = model_fn().to(device) # orginal model\n",
    "model1 = model_fn().to(device) # train with clean data\n",
    "model2 = model_fn().to(device) # train with new loss function\n",
    "model3 = model_fn().to(device)\n",
    "\n",
    "model0_sd = {k: v.clone().detach() for k, v in model1.state_dict().items()}\n",
    "\n",
    "optimizer0 = optim.SGD(model0.parameters(), lr=0.001)\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.001)\n",
    "optimizer3 = optim.SGD(model3.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.36, 2.32, 2.27, 2.24, 2.2, 2.16, 2.12, 2.09, 2.06, 2.02, 2.0, 1.97, 1.94, 1.91, 1.89, 1.86, 1.84, 1.81, 1.79, 1.77, 1.75, 1.74, 1.72, 1.7, 1.68, 1.67, 1.65, 1.63, 1.61, 1.6, 1.58, 1.57, 1.57, 1.55, 1.54, 1.52, 1.51, 1.5, 1.49, 1.47, 1.46, 1.45, 1.44, 1.43, 1.42, 1.4, 1.39, 1.38, 1.37, 1.36, 1.36, 1.35, 1.34, 1.33, 1.32, 1.31, 1.3, 1.29, 1.29, 1.28, 1.27, 1.26, 1.26, 1.25]\n",
      "model1_result {'test_accuracy': 0.7148}\n"
     ]
    }
   ],
   "source": [
    "# model1 train benign\n",
    "train_op(model1, client_loader, optimizer1, epochs=local_ep, print_train_loss=True)\n",
    "\n",
    "model1_sd = {key: value.clone() for key, value in model1.state_dict().items()}\n",
    "\n",
    "model1_result = eval_op_ensemble([model1], test_loader)\n",
    "print(\"model1_result\", model1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep0, loss_cs: -1.2648513317108154, loss_cos: 0.0, loss_obj: -0.6324256658554077\n",
      "ep0, loss_cs: -1.2789223194122314, loss_cos: 2.2292137145996094e-05, loss_obj: -0.6394500136375427\n",
      "ep0, loss_cs: -1.2330875396728516, loss_cos: 5.525350570678711e-05, loss_obj: -0.61651611328125\n",
      "ep0, loss_cs: -1.2704012393951416, loss_cos: 0.0001049041748046875, loss_obj: -0.6351481676101685\n",
      "ep0, loss_cs: -1.401615858078003, loss_cos: 0.00016438961029052734, loss_obj: -0.7007257342338562\n",
      "ep0, loss_cs: -1.2396160364151, loss_cos: 0.00020742416381835938, loss_obj: -0.6197043061256409\n",
      "ep0, loss_cs: -1.3055747747421265, loss_cos: 0.00024914738605730236, loss_obj: -0.6526628136634827\n",
      "ep0, loss_cs: -1.231921672821045, loss_cos: 0.0003154277801513672, loss_obj: -0.6158031225204468\n",
      "ep0, loss_cs: -1.2865276336669922, loss_cos: 0.00036072731018066406, loss_obj: -0.6430834531784058\n",
      "ep0, loss_cs: -1.1245653629302979, loss_cos: 0.00036036965320818126, loss_obj: -0.5621024966239929\n",
      "ep0, loss_cs: -1.1078349351882935, loss_cos: 0.00040096044540405273, loss_obj: -0.5537170171737671\n",
      "ep0, loss_cs: -1.2066411972045898, loss_cos: 0.0004360675811767578, loss_obj: -0.6031025648117065\n",
      "ep0, loss_cs: -1.351227879524231, loss_cos: 0.0004339217848610133, loss_obj: -0.6753969788551331\n",
      "ep0, loss_cs: -1.3352080583572388, loss_cos: 0.0005635619163513184, loss_obj: -0.6673222780227661\n",
      "ep0, loss_cs: -1.2464529275894165, loss_cos: 0.0007674098014831543, loss_obj: -0.6228427886962891\n",
      "ep0, loss_cs: -1.2872779369354248, loss_cos: 0.0008513330831192434, loss_obj: -0.6432133316993713\n",
      "ep0, loss_cs: -1.2918455600738525, loss_cos: 0.0011281371116638184, loss_obj: -0.645358681678772\n",
      "ep0, loss_cs: -1.2501921653747559, loss_cos: 0.0014271736145019531, loss_obj: -0.624382495880127\n",
      "ep0, loss_cs: -1.2582762241363525, loss_cos: 0.0015726088313385844, loss_obj: -0.6283518075942993\n",
      "ep0, loss_cs: -1.3213621377944946, loss_cos: 0.0018402932910248637, loss_obj: -0.6597609519958496\n",
      "ep0, loss_cs: -1.3919310569763184, loss_cos: 0.0021007657051086426, loss_obj: -0.6949151754379272\n",
      "ep0, loss_cs: -1.2088547945022583, loss_cos: 0.0025442836340516806, loss_obj: -0.603155255317688\n",
      "ep0, loss_cs: -1.3159537315368652, loss_cos: 0.0027616024017333984, loss_obj: -0.6565960645675659\n",
      "ep0, loss_cs: -1.334453821182251, loss_cos: 0.003207027679309249, loss_obj: -0.6656234264373779\n",
      "ep0, loss_cs: -1.394294023513794, loss_cos: 0.0036855933722108603, loss_obj: -0.6953042149543762\n",
      "ep0, loss_cs: -1.3093069791793823, loss_cos: 0.004015207290649414, loss_obj: -0.6526458859443665\n",
      "ep0, loss_cs: -1.3701472282409668, loss_cos: 0.0041321516036987305, loss_obj: -0.683007538318634\n",
      "ep0, loss_cs: -1.306405782699585, loss_cos: 0.004562020301818848, loss_obj: -0.6509218811988831\n",
      "ep0, loss_cs: -1.309553861618042, loss_cos: 0.005450725555419922, loss_obj: -0.652051568031311\n",
      "ep0, loss_cs: -1.4638197422027588, loss_cos: 0.0059937238693237305, loss_obj: -0.7289130091667175\n",
      "ep0, loss_cs: -1.4272589683532715, loss_cos: 0.006651937961578369, loss_obj: -0.710303544998169\n",
      "ep0, loss_cs: -1.2083688974380493, loss_cos: 0.007657110225409269, loss_obj: -0.6003559231758118\n",
      "ep0, loss_cs: -1.3406431674957275, loss_cos: 0.008265852928161621, loss_obj: -0.666188657283783\n",
      "ep0, loss_cs: -1.427177906036377, loss_cos: 0.008949815295636654, loss_obj: -0.7091140747070312\n",
      "ep0, loss_cs: -1.462287187576294, loss_cos: 0.009733915328979492, loss_obj: -0.7262766361236572\n",
      "ep0, loss_cs: -1.5136586427688599, loss_cos: 0.010677576065063477, loss_obj: -0.7514905333518982\n",
      "ep0, loss_cs: -1.340193748474121, loss_cos: 0.012224732898175716, loss_obj: -0.6639845371246338\n",
      "ep0, loss_cs: -1.5269681215286255, loss_cos: 0.013178706169128418, loss_obj: -0.7568947076797485\n",
      "ep0, loss_cs: -1.388789176940918, loss_cos: 0.014693379402160645, loss_obj: -0.6870478987693787\n",
      "ep0, loss_cs: -1.5042845010757446, loss_cos: 0.015978217124938965, loss_obj: -0.7441531419754028\n",
      "ep0, loss_cs: -1.6055822372436523, loss_cos: 0.017648935317993164, loss_obj: -0.7939666509628296\n",
      "ep0, loss_cs: -1.6564795970916748, loss_cos: 0.020152390003204346, loss_obj: -0.8181636333465576\n",
      "ep0, loss_cs: -1.6948782205581665, loss_cos: 0.022398829460144043, loss_obj: -0.8362396955490112\n",
      "ep0, loss_cs: -1.726374864578247, loss_cos: 0.025179563090205193, loss_obj: -0.8505976796150208\n",
      "ep0, loss_cs: -1.6814640760421753, loss_cos: 0.02826523594558239, loss_obj: -0.8265994191169739\n",
      "ep0, loss_cs: -1.5578421354293823, loss_cos: 0.031584616750478745, loss_obj: -0.7631287574768066\n",
      "ep0, loss_cs: -1.6167893409729004, loss_cos: 0.03505873680114746, loss_obj: -0.7908653020858765\n",
      "ep0, loss_cs: -1.5759996175765991, loss_cos: 0.03826415538787842, loss_obj: -0.7688677310943604\n",
      "ep0, loss_cs: -2.072596788406372, loss_cos: 0.042469799518585205, loss_obj: -1.0150635242462158\n",
      "ep0, loss_cs: -1.87985098361969, loss_cos: 0.0484347939491272, loss_obj: -0.915708065032959\n",
      "ep0, loss_cs: -1.607774257659912, loss_cos: 0.05400431156158447, loss_obj: -0.7768849730491638\n",
      "ep0, loss_cs: -2.0948941707611084, loss_cos: 0.05809729918837547, loss_obj: -1.018398404121399\n",
      "ep0, loss_cs: -1.8884501457214355, loss_cos: 0.06477916240692139, loss_obj: -0.9118354916572571\n",
      "ep0, loss_cs: -2.0000667572021484, loss_cos: 0.07055896520614624, loss_obj: -0.9647538661956787\n",
      "ep0, loss_cs: -2.407134532928467, loss_cos: 0.07865536212921143, loss_obj: -1.1642396450042725\n",
      "ep0, loss_cs: -1.8371624946594238, loss_cos: 0.08794688433408737, loss_obj: -0.8746078014373779\n",
      "ep0, loss_cs: -2.6868069171905518, loss_cos: 0.09444152563810349, loss_obj: -1.2961827516555786\n",
      "ep0, loss_cs: -2.2268669605255127, loss_cos: 0.10567766427993774, loss_obj: -1.0605946779251099\n",
      "ep0, loss_cs: -2.7275469303131104, loss_cos: 0.11461019515991211, loss_obj: -1.3064683675765991\n",
      "ep0, loss_cs: -2.6486144065856934, loss_cos: 0.12646901607513428, loss_obj: -1.2610726356506348\n",
      "ep0, loss_cs: -2.8698153495788574, loss_cos: 0.13843320310115814, loss_obj: -1.365691065788269\n",
      "ep0, loss_cs: -3.4020824432373047, loss_cos: 0.1512024998664856, loss_obj: -1.625440001487732\n",
      "ep0, loss_cs: -2.276811361312866, loss_cos: 0.16669481992721558, loss_obj: -1.055058240890503\n",
      "eval losses [1.25, 1.25, 1.26, 1.26, 1.27, 1.27, 1.27, 1.28, 1.29, 1.3, 1.31, 1.32, 1.33, 1.34, 1.36, 1.38, 1.4, 1.42, 1.45, 1.48, 1.51, 1.57, 1.64, 1.73, 1.82, 1.96, 2.1, 2.28, 2.49, 2.76, 3.08, 3.47]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': -1.5893410528512826}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model2 train with new loss function\n",
    "model2.load_state_dict(model1_sd)\n",
    "reverse_train_w_cos(model2, client_loader, optimizer2, epochs=1, \n",
    "                                model0_sd = model0_sd, \n",
    "                                model1_sd = model1_sd, \n",
    "                                beta = 0.5, \n",
    "                                budget = 0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1_2 cos dist tensor(0.1771, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cos_d_model1_2 = cos_dist(flat_dict(model1_sd) - flat_dict(model0_sd), \n",
    "                          flat_dict(model2.state_dict()) - flat_dict(model0_sd))\n",
    "print(\"model1_2 cos dist\", cos_d_model1_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model2_result {'test_accuracy': 0.1008}\n"
     ]
    }
   ],
   "source": [
    "model2_result = eval_op_ensemble([model2], test_loader)\n",
    "print(\"model2_result\", model2_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
